{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1690185948098,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"VhJqAwd6nL-N"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":165,"status":"ok","timestamp":1690185949195,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"x-BYrGLLnbHy"},"outputs":[],"source":["data = pd.read_csv('/content/drive/MyDrive/CodePractice/10. Hyperparameter/diabetes.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":174,"status":"ok","timestamp":1690180689976,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"1iOCvmPKnbKb","outputId":"88de6b07-1fd6-4601-9f39-229e6507336c"},"outputs":[{"data":{"text/html":["\n","\n","  <div id=\"df-4821e07c-688a-4196-83fa-6fe8e9c9120e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4821e07c-688a-4196-83fa-6fe8e9c9120e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-14c22ce6-714f-431b-9ec1-3e44fa145355\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14c22ce6-714f-431b-9ec1-3e44fa145355')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-14c22ce6-714f-431b-9ec1-3e44fa145355 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4821e07c-688a-4196-83fa-6fe8e9c9120e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4821e07c-688a-4196-83fa-6fe8e9c9120e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"vdXPhnS1oNve"},"source":["There are lots of 0 values in col you can see like insulin is 0 which is not possible so we can replce this values with mean. In DL is not major problem but in Ml it is big prblem"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":134,"status":"ok","timestamp":1690185955850,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"9vCe1pVtnbO7"},"outputs":[],"source":["df  = data.copy()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1690180721834,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"k4rvD5BTnbRv","outputId":"b6930bf8-f0d4-4106-99fd-a8a13244027e"},"outputs":[{"data":{"text/plain":["Pregnancies                 0\n","Glucose                     0\n","BloodPressure               0\n","SkinThickness               0\n","Insulin                     0\n","BMI                         0\n","DiabetesPedigreeFunction    0\n","Age                         0\n","Outcome                     0\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1690180786611,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"WkeQtfmunbUU","outputId":"68ab6f27-10f8-4ecf-f14b-ead8c97e8c9f"},"outputs":[{"data":{"text/plain":["Pregnancies                 0.221898\n","Glucose                     0.466581\n","BloodPressure               0.065068\n","SkinThickness               0.074752\n","Insulin                     0.130548\n","BMI                         0.292695\n","DiabetesPedigreeFunction    0.173844\n","Age                         0.238356\n","Outcome                     1.000000\n","Name: Outcome, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.corr()[\"Outcome\"]"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1690185960876,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"sEqUHea_n-NN"},"outputs":[],"source":["x = df.iloc[:, :-1]\n","y = df.iloc[:,-1]"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":869,"status":"ok","timestamp":1690185969014,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"7r_Qn93JoEbl"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","x = scaler.fit_transform(x)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1690185970234,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"_Gy-kr6btA5-"},"outputs":[],"source":["# split the data into training and test\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)"]},{"cell_type":"markdown","metadata":{"id":"q3Z3bCQttEmd"},"source":["# build model now"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3830,"status":"ok","timestamp":1690185976696,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"_TrmbLKhtC25"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Dropout"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":167,"status":"ok","timestamp":1690185977761,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"zAAAAocZtYGm"},"outputs":[],"source":["model = Sequential()\n","model.add(Dense(units = 32, activation = \"relu\", input_dim = 8))\n","model.add(Dense(1,activation = \"sigmoid\"))\n","model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics= [\"accuracy\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2335,"status":"ok","timestamp":1690185984059,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"2BRzAnCYuIZ2","outputId":"d78c86f6-c6b1-498f-ac9e-6cdf6320bbd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","20/20 [==============================] - 1s 17ms/step - loss: 0.8086 - accuracy: 0.3567 - val_loss: 0.7438 - val_accuracy: 0.3571\n","Epoch 2/10\n","20/20 [==============================] - 0s 6ms/step - loss: 0.7355 - accuracy: 0.4528 - val_loss: 0.6863 - val_accuracy: 0.5584\n","Epoch 3/10\n","20/20 [==============================] - 0s 5ms/step - loss: 0.6800 - accuracy: 0.5814 - val_loss: 0.6424 - val_accuracy: 0.6494\n","Epoch 4/10\n","20/20 [==============================] - 0s 4ms/step - loss: 0.6370 - accuracy: 0.6580 - val_loss: 0.6086 - val_accuracy: 0.6948\n","Epoch 5/10\n","20/20 [==============================] - 0s 4ms/step - loss: 0.6031 - accuracy: 0.7134 - val_loss: 0.5823 - val_accuracy: 0.6948\n","Epoch 6/10\n","20/20 [==============================] - 0s 4ms/step - loss: 0.5746 - accuracy: 0.7166 - val_loss: 0.5605 - val_accuracy: 0.6948\n","Epoch 7/10\n","20/20 [==============================] - 0s 4ms/step - loss: 0.5520 - accuracy: 0.7296 - val_loss: 0.5435 - val_accuracy: 0.7078\n","Epoch 8/10\n","20/20 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7378 - val_loss: 0.5291 - val_accuracy: 0.7338\n","Epoch 9/10\n","20/20 [==============================] - 0s 5ms/step - loss: 0.5192 - accuracy: 0.7476 - val_loss: 0.5161 - val_accuracy: 0.7403\n","Epoch 10/10\n","20/20 [==============================] - 0s 5ms/step - loss: 0.5079 - accuracy: 0.7492 - val_loss: 0.5058 - val_accuracy: 0.7468\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7c46f2893280>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"rkgoFordu333"},"source":["# HyperParameters Tunning - Keras tuner\n","\n","### 1) How to select appropriate optimizer\n","### 2) No. of nodes in a layer\n","### 3) How to select No of hidden layer\n","### 4) All in all with one model"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6618,"status":"ok","timestamp":1690182752084,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"W03iyNbHuyXN","outputId":"0887f1b0-001d-49be-b815-b6925e23b95f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/176.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m174.1/176.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# !pip install keras-tuner -q # first install it"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1690185999413,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"hGBRfljCvfrZ"},"outputs":[],"source":["import keras_tuner as kt"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690183156649,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"j98fJ_H2vtuR"},"outputs":[],"source":["# Optimizer\n","def build_model(hp):\n","  model = Sequential()\n","  model.add(Dense(32, activation='relu', input_dim=8))\n","  model.add(Dense(1, activation='sigmoid'))\n","  optimizer = hp.Choice(\"optimizer\",values = [\"adam\",\"sgd\",\"rmsprop\",\"adadelta\"])\n","  model.compile(optimizer=optimizer,loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n","  return model"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":205,"status":"ok","timestamp":1690183314073,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"bI1oV4cWxC3g"},"outputs":[],"source":["# Best Optimization\n","tuner = kt.RandomSearch(build_model, objective = \"val_accuracy\", max_trials = 5)"]},{"cell_type":"markdown","metadata":{"id":"WCR8BFJ_zrKm"},"source":["1. objective: The objective of the hyperparameter search is to maximize the validation accuracy. In this case, the \"val_accuracy\" is specified as the objective, which means the tuner will try to find hyperparameters that lead to higher validation accuracy.\n","\n","2. max_trials: This parameter determines the total number of hyperparameter combinations to try during the search. In this case, the tuner will explore 5 different combinations of hyperparameters."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8753,"status":"ok","timestamp":1690183409208,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"AWAoZWWixn8l","outputId":"eb88c24f-a665-4b41-85d9-ea3dcb77e4c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 4 Complete [00h 00m 03s]\n","val_accuracy: 0.7597402334213257\n","\n","Best val_accuracy So Far: 0.7597402334213257\n","Total elapsed time: 00h 00m 09s\n"]}],"source":["tuner.search(x_train,y_train, epochs = 5 , validation_data = (x_test,y_test))\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690183587487,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"9NxAapaayphR","outputId":"1d040601-8298-4835-dc04-fcf3948fd44f"},"outputs":[{"data":{"text/plain":["[<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters at 0x7a00c4902b30>]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tuner.get_best_hyperparameters()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1690183596845,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"LV6q5LFrytVw","outputId":"adc59d60-f00e-47d5-b66e-570799eb993b"},"outputs":[{"data":{"text/plain":["{'optimizer': 'adam'}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tuner.get_best_hyperparameters()[0].values"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1690183655898,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"sfcotmvbyvi8"},"outputs":[],"source":["# adam is working better so we got the optimizer that we have to use"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":524,"status":"ok","timestamp":1690184075870,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"T33LXJpCy9_I"},"outputs":[],"source":["model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"markdown","metadata":{"id":"SAp5ultm0QUP"},"source":["1. The line of code model = tuner.get_best_models(num_models=1)[0] is used to retrieve the best model found during the hyperparameter tuning process using Keras Tuner.\n","\n","2. get_best_models(num_models=1): This method is used to retrieve the best models found during the hyperparameter tuning process. The num_models parameter specifies the number of best models to retrieve. In this case, num_models=1, meaning only the top-performing model will be returned.\n","\n","3. [0]: The result of tuner.get_best_models(num_models=1) is a list containing the best model(s). Since we are interested in just the top-performing model, we use indexing [0] to access the first (and only) element in the list."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1690184203299,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"1rbl6gcf0lSt","outputId":"285c9f0b-f5c6-4616-f5f7-392111586bac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_4 (Dense)             (None, 32)                288       \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 321\n","Trainable params: 321\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"i8iNM3f11MGI"},"source":["## Lets look for neuron ?"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":165,"status":"ok","timestamp":1690184561476,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"pgvnNzo_1DhW"},"outputs":[],"source":["def build_model(hp):\n","  model = Sequential()\n","  units = hp.Int('units',8,128, step=8)\n","  # 8- lower limit or min neuron\n","  # 128 - upper limit - max neuron\n","  model.add(Dense(units=units, activation ='relu', input_dim=8))\n","  model.add(Dense(1, activation='sigmoid'))\n","  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","  return model"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690184562169,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"AkCx8x102DCd"},"outputs":[],"source":["# Tunner object\n","tuner = kt.RandomSearch(build_model, objective = 'val_accuracy', max_trials=5, directory='mydir', project_name='neuron')"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1690184562312,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"jMDUBY6C2LIs"},"outputs":[],"source":["tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690184646442,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"-jH0vENS2RhQ","outputId":"22b46374-072c-4680-fafa-7c63378b9d81"},"outputs":[{"data":{"text/plain":["{'units': 112}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["tuner.get_best_hyperparameters()[0].values"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":698,"status":"ok","timestamp":1690184656550,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"NAC53FgM2v0m"},"outputs":[],"source":["model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16526,"status":"ok","timestamp":1690184737574,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"r7w8GLhp2yKM","outputId":"0c20dd84-8761-4401-ba94-d97fc0fc689a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 7/100\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<tf.Variable 'Adam/m/dense/kernel:0' shape=(8, 112) dtype=float32, numpy=\n","array([[3.13116398e-05, 8.77908951e-06, 2.19744383e-04, 1.74236076e-04,\n","        4.78479114e-06, 6.60026853e-05, 6.01793317e-07, 7.52079904e-06,\n","        9.64353094e-05, 1.76762987e-05, 8.11001592e-05, 4.38481402e-05,\n","        1.17367439e-04, 2.39737827e-04, 2.93880767e-05, 5.65166665e-05,\n","        1.46383463e-05, 1.45976359e-04, 1.62877783e-04, 1.11026828e-04,\n","        8.22112779e-05, 3.64987005e-04, 1.03591636e-04, 6.82549507e-05,\n","        4.26429324e-06, 3.82843018e-06, 1.65569600e-06, 2.71840690e-04,\n","        7.69754115e-05, 6.22428479e-06, 1.67152699e-04, 4.20406577e-05,\n","        2.52029076e-05, 3.96193122e-04, 2.21753435e-04, 2.72824836e-05,\n","        1.30858625e-05, 5.38654458e-05, 3.40774830e-04, 2.05805122e-07,\n","        2.82144701e-05, 4.28176500e-05, 2.83356599e-07, 1.51767699e-05,\n","        9.36512224e-05, 1.54819536e-05, 9.38527228e-06, 1.64820249e-05,\n","        2.24461655e-06, 3.24737812e-06, 2.55309278e-04, 2.21548253e-05,\n","        2.58803513e-04, 3.49543698e-04, 2.47834629e-04, 7.03455225e-05,\n","        9.06947243e-05, 1.55637190e-06, 2.28097888e-05, 2.40189227e-04,\n","        3.15222060e-05, 8.34852835e-05, 5.66610324e-05, 3.73840041e-04,\n","        4.68262806e-06, 2.32723232e-05, 3.32114541e-05, 1.82530799e-04,\n","        2.22940271e-05, 1.41974378e-05, 6.28254347e-05, 2.66742518e-07,\n","        4.07347325e-05, 2.36270444e-05, 1.95663597e-05, 2.06791141e-04,\n","        1.13511815e-04, 9.30975439e-05, 1.83136544e-05, 4.15175964e-05,\n","        4.78044342e-07, 5.43901915e-05, 6.91730747e-07, 1.19987249e-04,\n","        1.45500846e-04, 2.66383231e-05, 2.64827286e-05, 2.45179945e-05,\n","        1.30895991e-04, 1.05830168e-06, 1.35757466e-04, 8.36622785e-05,\n","        5.27784505e-05, 7.59928880e-05, 7.77525565e-05, 8.76488593e-06,\n","        4.44071251e-04, 2.25198470e-04, 3.27819471e-05, 1.10887356e-06,\n","        6.33227246e-05, 2.58283311e-04, 1.37239467e-05, 8.48205091e-05,\n","        9.08743386e-05, 2.61897338e-04, 1.82428645e-04, 6.58152130e-05,\n","        3.36960191e-04, 5.74164951e-05, 5.88165267e-06, 7.17880466e-05],\n","       [2.32747389e-05, 2.23084480e-05, 1.52731285e-04, 1.16372416e-04,\n","        1.00509342e-05, 7.33705456e-05, 1.95953362e-06, 1.60649197e-05,\n","        8.42964437e-05, 5.51184239e-05, 9.30007373e-05, 1.57700095e-04,\n","        4.21806508e-05, 9.92738496e-05, 1.60830186e-05, 1.67938793e-04,\n","        1.80177849e-05, 9.43517734e-05, 5.54044891e-05, 1.32469271e-04,\n","        1.00933335e-04, 1.99731905e-04, 1.66439422e-04, 6.94035698e-05,\n","        1.06630432e-05, 5.32395643e-06, 7.40861424e-07, 1.23799997e-04,\n","        1.17860756e-04, 1.60905929e-05, 1.21381578e-04, 9.47083681e-05,\n","        6.02403634e-05, 2.06357276e-04, 1.60597046e-04, 1.07105725e-05,\n","        1.65344500e-05, 8.16342654e-05, 1.25100734e-04, 1.68660819e-07,\n","        1.38810710e-05, 7.30923275e-05, 9.26572170e-08, 7.41727717e-06,\n","        3.05032627e-05, 7.86469809e-06, 1.09597204e-05, 1.20019249e-05,\n","        2.23488178e-06, 5.99646273e-06, 9.42548359e-05, 9.88631109e-06,\n","        1.40587566e-04, 1.54627487e-04, 1.13013695e-04, 8.36530671e-05,\n","        4.63605575e-05, 2.26983457e-06, 6.22527150e-05, 8.48847267e-05,\n","        1.81157811e-05, 6.18500635e-05, 2.95253139e-05, 2.11564911e-04,\n","        7.76732850e-06, 3.59005207e-05, 2.33533901e-05, 8.55104809e-05,\n","        1.23393065e-05, 1.02864105e-05, 1.74855973e-04, 8.70413771e-07,\n","        1.30123008e-05, 5.86528695e-05, 1.09029961e-05, 1.10052206e-04,\n","        3.88870467e-05, 1.85032783e-04, 2.36986489e-05, 1.28500280e-04,\n","        6.13440989e-07, 1.75507215e-04, 4.03422519e-07, 2.92358836e-05,\n","        1.70533793e-04, 1.41047167e-05, 1.97485679e-05, 2.36808555e-05,\n","        3.43195308e-04, 3.79828612e-06, 6.28499183e-05, 3.05312606e-05,\n","        6.76405543e-05, 1.15940456e-04, 6.41201623e-05, 1.38854684e-05,\n","        1.63479766e-04, 9.78341559e-05, 2.81802913e-05, 4.97994506e-07,\n","        2.05065262e-05, 1.92530919e-04, 2.40147601e-05, 5.44721224e-05,\n","        3.29888826e-05, 1.58947747e-04, 8.10446290e-05, 1.47174083e-04,\n","        9.61002079e-05, 6.37337944e-05, 2.14398597e-06, 1.76031972e-04],\n","       [1.60337040e-05, 2.34603904e-05, 1.08993809e-04, 7.40636169e-05,\n","        1.09773318e-05, 1.45275131e-04, 2.32687080e-06, 2.10881662e-05,\n","        6.32810916e-05, 2.71484896e-05, 2.17925102e-04, 2.49636214e-04,\n","        1.01522281e-04, 1.73827735e-04, 1.15998646e-05, 1.41346129e-04,\n","        2.93555422e-05, 7.01937679e-05, 1.35278824e-04, 5.19337664e-05,\n","        1.07511725e-04, 1.91543877e-04, 1.89916362e-04, 2.08937727e-05,\n","        1.31931829e-05, 1.80506549e-06, 4.25489134e-07, 2.70404707e-04,\n","        1.37079784e-04, 2.19964058e-05, 9.52627379e-05, 1.09555833e-04,\n","        1.87526275e-05, 1.37823954e-04, 1.24333048e-04, 2.38389603e-05,\n","        2.39841665e-05, 1.02576989e-04, 8.53133388e-05, 6.07750010e-08,\n","        9.61874503e-06, 9.32139665e-05, 6.27953511e-08, 1.35132395e-05,\n","        1.89057191e-05, 1.04780102e-05, 3.46473098e-06, 5.43803799e-06,\n","        4.80483095e-06, 3.40860379e-06, 2.09844366e-04, 1.69655814e-05,\n","        2.85823655e-04, 1.04961822e-04, 2.10325961e-04, 1.32711895e-04,\n","        2.97159859e-05, 1.61870412e-06, 7.16324648e-05, 1.45351703e-04,\n","        7.43977034e-06, 1.20646488e-04, 2.21369246e-05, 1.61038857e-04,\n","        4.01184525e-06, 9.13596086e-05, 2.64696810e-05, 5.72162535e-05,\n","        3.63368304e-06, 5.02041939e-06, 2.53605453e-04, 1.11733232e-06,\n","        9.80170171e-06, 9.24661872e-05, 1.70042767e-05, 8.31452853e-05,\n","        3.47867535e-05, 1.95163550e-04, 1.70115909e-05, 2.15728462e-04,\n","        9.20364698e-07, 2.20909278e-04, 1.56168994e-07, 5.22004702e-05,\n","        7.35712383e-05, 7.88138095e-06, 3.53705582e-05, 4.55951013e-05,\n","        3.23232525e-04, 2.64788287e-06, 7.77578680e-05, 4.49350882e-05,\n","        8.18925109e-05, 1.48249135e-04, 5.61660745e-05, 5.78350500e-06,\n","        8.55219623e-05, 1.37382318e-04, 1.04301180e-05, 2.73205359e-07,\n","        1.39191434e-05, 1.75743509e-04, 1.42784920e-05, 7.04105187e-05,\n","        4.18510281e-05, 1.01965466e-04, 1.30414861e-04, 1.53759524e-04,\n","        1.92661333e-04, 3.53128999e-05, 4.98170493e-06, 2.39895104e-04],\n","       [4.25624748e-05, 2.19078611e-05, 2.70825432e-04, 1.58047391e-04,\n","        8.57580471e-06, 9.48928646e-05, 1.43354407e-06, 1.06164971e-05,\n","        1.01157799e-04, 3.13965174e-05, 1.21631514e-04, 1.56802576e-04,\n","        7.39892930e-05, 8.59156717e-05, 1.62396245e-05, 1.89012906e-04,\n","        1.52137109e-05, 1.17773117e-04, 1.14818409e-04, 1.33032387e-04,\n","        9.45780121e-05, 3.30065202e-04, 1.56897207e-04, 4.59430630e-05,\n","        9.23141033e-06, 3.80062443e-06, 5.55612473e-07, 1.64924568e-04,\n","        1.19195378e-04, 1.16789433e-05, 1.80414936e-04, 6.89261142e-05,\n","        4.71478743e-05, 4.26550745e-04, 1.83998112e-04, 1.75233181e-05,\n","        1.41452119e-05, 8.07654869e-05, 2.38993685e-04, 1.73301714e-07,\n","        2.63613656e-05, 6.70714362e-05, 1.45032516e-07, 5.69995609e-06,\n","        5.42247144e-05, 1.08437189e-05, 6.10948928e-06, 1.10234878e-05,\n","        2.85190367e-06, 5.18231855e-06, 1.45897109e-04, 8.98755388e-06,\n","        2.81573331e-04, 2.16941349e-04, 9.70966576e-05, 8.81792294e-05,\n","        5.89355768e-05, 1.49994753e-06, 5.65628725e-05, 8.28876073e-05,\n","        1.88141694e-05, 7.38259696e-05, 3.68628498e-05, 2.70547462e-04,\n","        5.10413201e-06, 2.72746001e-05, 3.57119243e-05, 7.82584902e-05,\n","        1.51189815e-05, 1.10444971e-05, 1.10961970e-04, 7.97602638e-07,\n","        2.00963987e-05, 3.72799695e-05, 6.54788846e-06, 1.46430684e-04,\n","        5.83357541e-05, 1.82011878e-04, 2.91056404e-05, 1.29181441e-04,\n","        5.55163581e-07, 1.25467544e-04, 6.13649320e-07, 5.00258138e-05,\n","        2.05712407e-04, 1.07300966e-05, 2.51634774e-05, 2.47424905e-05,\n","        2.41136309e-04, 2.82695669e-06, 7.67457968e-05, 5.28814853e-05,\n","        9.49504247e-05, 9.50194080e-05, 1.03597529e-04, 1.24263979e-05,\n","        1.77972921e-04, 1.68410988e-04, 1.51637141e-05, 8.73990587e-07,\n","        1.69754221e-05, 2.93501216e-04, 2.09586397e-05, 5.40321598e-05,\n","        6.73632749e-05, 2.01500181e-04, 9.31988252e-05, 1.16997006e-04,\n","        2.00121736e-04, 5.39917892e-05, 2.92950244e-06, 8.26693140e-05],\n","       [1.97572008e-05, 1.23713498e-05, 9.48796805e-05, 3.76693024e-05,\n","        3.49101128e-06, 6.65816260e-05, 1.61894616e-06, 1.87791265e-05,\n","        9.77132731e-05, 5.22948612e-05, 1.09495195e-04, 1.37974130e-04,\n","        2.62989433e-05, 5.22766168e-05, 1.34785023e-05, 1.66389014e-04,\n","        2.13488584e-05, 6.34326425e-05, 3.88551089e-05, 6.59937577e-05,\n","        9.48316883e-05, 1.41373748e-04, 1.08854176e-04, 4.53978500e-05,\n","        3.90661990e-06, 6.90054185e-06, 7.97780785e-07, 2.38310473e-04,\n","        1.16647083e-04, 6.20554601e-06, 1.18005977e-04, 4.84503071e-05,\n","        8.87599308e-05, 1.47492872e-04, 8.49042117e-05, 8.63130299e-06,\n","        4.83719623e-06, 3.57201425e-05, 5.18096494e-05, 1.35275315e-07,\n","        2.69200882e-05, 4.66503152e-05, 9.07384390e-08, 3.06037600e-06,\n","        1.34400916e-05, 1.53946064e-06, 7.47260765e-06, 6.98960230e-06,\n","        1.05871129e-06, 2.17922593e-06, 5.89412339e-05, 1.62532760e-05,\n","        6.43811218e-05, 6.57387063e-05, 7.77470195e-05, 1.12668298e-04,\n","        4.32717752e-05, 1.17679156e-06, 7.22384211e-05, 1.20252822e-04,\n","        1.17289683e-05, 3.18181628e-05, 2.89952441e-05, 1.08942993e-04,\n","        7.46702153e-06, 2.22414146e-05, 1.64578050e-05, 7.26035505e-05,\n","        8.19946308e-06, 4.66096162e-06, 1.60961776e-04, 6.36228776e-07,\n","        9.44536350e-06, 7.03682381e-05, 5.79860080e-06, 1.10119545e-04,\n","        5.40056099e-05, 6.69344226e-05, 8.71309931e-06, 1.40328950e-04,\n","        2.76443473e-07, 6.36466866e-05, 1.51202471e-07, 2.18305613e-05,\n","        1.20230223e-04, 1.56529477e-05, 4.92106938e-06, 8.53693746e-06,\n","        1.19242948e-04, 3.10746009e-06, 2.83250392e-05, 1.92321113e-05,\n","        6.31009170e-05, 3.55121811e-05, 5.90849559e-05, 1.12248890e-05,\n","        9.70897381e-05, 4.55646405e-05, 2.07151043e-05, 3.37436347e-07,\n","        1.80094139e-05, 1.69052830e-04, 2.70308246e-05, 5.73173966e-05,\n","        3.05390931e-05, 6.93416659e-05, 6.09602394e-05, 1.70160929e-04,\n","        7.16848444e-05, 6.08244954e-05, 1.21615960e-06, 1.12893256e-04],\n","       [2.45772480e-05, 1.61806784e-05, 1.73595748e-04, 1.28365005e-04,\n","        5.73724174e-06, 9.17070720e-05, 1.43236639e-06, 1.31893185e-05,\n","        8.40903012e-05, 4.22167504e-05, 1.17334937e-04, 2.18418718e-04,\n","        5.27674492e-05, 6.61298109e-05, 1.64210778e-05, 2.04018332e-04,\n","        2.20847760e-05, 8.66273767e-05, 3.15027064e-05, 2.15559543e-04,\n","        1.62652461e-04, 2.34861072e-04, 2.38519220e-04, 5.24600509e-05,\n","        5.67579173e-06, 5.12354472e-06, 4.23040206e-07, 1.07593078e-04,\n","        9.89170803e-05, 1.12845109e-05, 1.53111047e-04, 6.15217432e-05,\n","        5.28573401e-05, 2.34780426e-04, 1.32178640e-04, 1.25240658e-05,\n","        1.30470444e-05, 4.00023710e-05, 1.39966098e-04, 2.19738155e-07,\n","        1.61942990e-05, 1.13676571e-04, 7.19636191e-08, 1.17130276e-05,\n","        4.45026308e-05, 7.85330394e-06, 6.73705017e-06, 9.86378745e-06,\n","        4.34461344e-06, 4.63387732e-06, 6.32121664e-05, 8.92270418e-06,\n","        2.30244957e-04, 1.93425018e-04, 7.26864600e-05, 7.59874456e-05,\n","        5.06173019e-05, 2.11961128e-06, 6.06291906e-05, 5.79757434e-05,\n","        1.18469179e-05, 8.23908777e-05, 2.42429360e-05, 1.77677735e-04,\n","        6.12069653e-06, 3.92292241e-05, 1.90515384e-05, 1.10463574e-04,\n","        6.91271907e-06, 7.44338104e-06, 1.22568526e-04, 6.40607027e-07,\n","        5.06955575e-06, 2.03949894e-05, 4.99561656e-06, 1.14292212e-04,\n","        6.19820858e-05, 1.32528949e-04, 4.04456878e-05, 2.03637886e-04,\n","        3.02087358e-07, 1.05672945e-04, 3.39612399e-07, 1.17427353e-05,\n","        2.06477052e-04, 1.12115058e-05, 2.61121531e-05, 4.91231767e-05,\n","        1.47569939e-04, 2.91430615e-06, 2.94810052e-05, 1.60081290e-05,\n","        8.51454897e-05, 1.42372621e-04, 9.43033956e-05, 1.35991486e-05,\n","        2.21196155e-04, 5.11989638e-05, 2.18054047e-05, 5.32320541e-07,\n","        1.81768937e-05, 1.47270868e-04, 2.29436646e-05, 4.29519350e-05,\n","        6.09397684e-05, 1.01266043e-04, 4.88169353e-05, 8.89112562e-05,\n","        7.92994761e-05, 6.25686298e-05, 2.42342662e-06, 1.04591876e-04],\n","       [2.45336014e-05, 2.19230169e-05, 1.37429350e-04, 1.45502767e-04,\n","        1.89592924e-06, 7.47515733e-05, 1.02323440e-06, 9.91664456e-06,\n","        5.66142444e-05, 1.13407277e-05, 7.69746839e-05, 9.88344764e-05,\n","        4.68697035e-05, 1.41671349e-04, 1.44672449e-05, 1.23814272e-04,\n","        2.00221202e-05, 5.72992649e-05, 3.53906980e-05, 2.09487014e-04,\n","        7.41009062e-05, 3.59421421e-04, 9.44646745e-05, 6.94598566e-05,\n","        6.10402731e-06, 4.44122998e-06, 3.69411168e-07, 8.53802630e-05,\n","        5.01262839e-05, 1.91064214e-06, 1.52332694e-04, 7.97788089e-05,\n","        2.11388215e-05, 1.81223702e-04, 5.10985701e-05, 8.37496191e-06,\n","        1.50385258e-05, 7.58384049e-05, 1.15678180e-04, 1.95545198e-07,\n","        1.44749320e-05, 4.25672006e-05, 2.74645906e-08, 7.87271711e-06,\n","        4.04594866e-05, 7.69744656e-06, 1.10541478e-05, 1.96125784e-05,\n","        1.36620508e-06, 1.79005337e-06, 7.96416134e-05, 4.54843348e-06,\n","        1.57231814e-04, 1.87909929e-04, 1.44547244e-04, 1.92985517e-05,\n","        5.57821768e-05, 6.54328574e-07, 5.94780759e-05, 3.47950918e-05,\n","        4.82006544e-06, 4.42331075e-05, 3.04047408e-05, 1.90254301e-04,\n","        2.65681683e-06, 5.07375371e-05, 2.63538823e-05, 1.78324815e-04,\n","        2.87665966e-06, 6.94278970e-06, 3.79694648e-05, 3.42227196e-07,\n","        5.39244138e-06, 1.40861384e-05, 1.06830576e-05, 1.23660677e-04,\n","        5.84723748e-05, 7.66985686e-05, 1.41729342e-05, 6.50786315e-05,\n","        1.20759324e-07, 1.24694314e-04, 4.15759700e-07, 1.60213513e-05,\n","        1.54816327e-04, 1.66555783e-05, 2.89980526e-05, 1.06752941e-05,\n","        1.74350513e-04, 1.02774391e-06, 1.72663804e-05, 7.73993997e-06,\n","        7.80906048e-05, 7.52169362e-05, 7.78605390e-05, 1.59268329e-05,\n","        2.35160740e-04, 2.05117321e-05, 3.12722186e-05, 5.02485932e-07,\n","        4.68055296e-05, 2.01059142e-04, 1.68639799e-05, 6.10051866e-05,\n","        5.06787328e-05, 1.50312873e-04, 9.54545685e-05, 9.39345191e-05,\n","        4.27078994e-05, 1.94121840e-05, 2.57881652e-06, 8.93751640e-05],\n","       [1.83443881e-05, 1.60756263e-05, 1.45881218e-04, 7.99874833e-05,\n","        7.33831348e-06, 4.94237975e-05, 3.16130127e-06, 1.87330188e-05,\n","        3.43112370e-05, 2.71778426e-05, 5.42924536e-05, 1.13068585e-04,\n","        6.50577058e-05, 1.36800649e-04, 1.48591780e-05, 2.25052092e-04,\n","        1.06779362e-05, 6.48978821e-05, 7.93067156e-05, 1.37604409e-04,\n","        7.34168498e-05, 1.43714788e-04, 1.34116708e-04, 3.64402949e-05,\n","        8.32576643e-06, 4.32940396e-06, 9.08705033e-07, 8.75684782e-05,\n","        7.81602648e-05, 1.31776669e-05, 9.65549771e-05, 6.07217698e-05,\n","        3.94450399e-05, 1.69365914e-04, 1.62767552e-04, 1.09455395e-05,\n","        9.40400059e-06, 4.20167598e-05, 1.35952912e-04, 1.38011814e-07,\n","        1.44694422e-05, 6.26159599e-05, 9.50125596e-08, 1.05927957e-05,\n","        3.68458968e-05, 5.46004594e-06, 5.96079644e-06, 1.19915212e-05,\n","        2.70774331e-06, 8.37703010e-06, 1.15548362e-04, 4.22811354e-06,\n","        7.09879532e-05, 1.95170694e-04, 8.17634718e-05, 8.24072049e-05,\n","        3.80549209e-05, 1.53431802e-06, 4.55907830e-05, 1.11980000e-04,\n","        9.66049174e-06, 8.16483516e-05, 1.43119187e-05, 1.68778453e-04,\n","        5.42446469e-06, 6.12791264e-05, 2.65597992e-05, 8.57589621e-05,\n","        5.41790359e-06, 6.67630820e-06, 1.03867998e-04, 7.46202033e-07,\n","        1.94487657e-05, 1.87473397e-05, 9.80186815e-06, 8.50628276e-05,\n","        3.29424729e-05, 2.10021695e-04, 3.24415923e-05, 3.46164416e-05,\n","        4.82445159e-07, 1.53530957e-04, 4.05495655e-07, 4.77330395e-05,\n","        1.28915854e-04, 1.47996770e-05, 1.92572261e-05, 2.82806323e-05,\n","        2.88748502e-04, 3.02921103e-06, 4.29937027e-05, 1.59430256e-05,\n","        3.30852708e-05, 1.30516084e-04, 7.55371802e-05, 1.06852976e-05,\n","        2.34646737e-04, 1.00471472e-04, 1.75201949e-05, 5.46870922e-07,\n","        2.63038710e-05, 1.89214406e-04, 8.67626113e-06, 1.68197712e-05,\n","        5.44950381e-05, 1.90089602e-04, 8.46281546e-05, 1.05192797e-04,\n","        1.31871231e-04, 6.33937379e-05, 3.14285739e-06, 8.78434730e-05]],\n","      dtype=float32)> and <tf.Variable 'Adam/v/dense/kernel:0' shape=(8, 112) dtype=float32, numpy=\n","array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","      dtype=float32)>).\n","WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<tf.Variable 'Adam/v/dense/kernel:0' shape=(8, 112) dtype=float32, numpy=\n","array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","      dtype=float32)> and <tf.Variable 'Adam/v/dense/bias:0' shape=(112,) dtype=float32, numpy=\n","array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>).\n","WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<tf.Variable 'Adam/m/dense/bias:0' shape=(112,) dtype=float32, numpy=\n","array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)> and <tf.Variable 'Adam/v/dense_1/kernel:0' shape=(112, 1) dtype=float32, numpy=\n","array([[0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.]], dtype=float32)>).\n","WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<tf.Variable 'Adam/v/dense/bias:0' shape=(112,) dtype=float32, numpy=\n","array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)> and <tf.Variable 'Adam/v/dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>).\n"]},{"name":"stdout","output_type":"stream","text":["20/20 [==============================] - 2s 15ms/step - loss: 0.4912 - accuracy: 0.7655 - val_loss: 0.4659 - val_accuracy: 0.7727\n","Epoch 8/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.7687 - val_loss: 0.4543 - val_accuracy: 0.7922\n","Epoch 9/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4655 - accuracy: 0.7704 - val_loss: 0.4457 - val_accuracy: 0.7987\n","Epoch 10/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4605 - accuracy: 0.7834 - val_loss: 0.4393 - val_accuracy: 0.7987\n","Epoch 11/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.7866 - val_loss: 0.4381 - val_accuracy: 0.7922\n","Epoch 12/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.7866 - val_loss: 0.4374 - val_accuracy: 0.7987\n","Epoch 13/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4526 - accuracy: 0.7850 - val_loss: 0.4381 - val_accuracy: 0.8052\n","Epoch 14/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4523 - accuracy: 0.7850 - val_loss: 0.4363 - val_accuracy: 0.7922\n","Epoch 15/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4505 - accuracy: 0.7866 - val_loss: 0.4365 - val_accuracy: 0.7922\n","Epoch 16/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4478 - accuracy: 0.7899 - val_loss: 0.4380 - val_accuracy: 0.7857\n","Epoch 17/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4456 - accuracy: 0.7899 - val_loss: 0.4374 - val_accuracy: 0.7922\n","Epoch 18/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.7948 - val_loss: 0.4334 - val_accuracy: 0.7727\n","Epoch 19/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4429 - accuracy: 0.7948 - val_loss: 0.4382 - val_accuracy: 0.7987\n","Epoch 20/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4428 - accuracy: 0.7964 - val_loss: 0.4372 - val_accuracy: 0.7792\n","Epoch 21/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4405 - accuracy: 0.7948 - val_loss: 0.4393 - val_accuracy: 0.7922\n","Epoch 22/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4391 - accuracy: 0.7915 - val_loss: 0.4407 - val_accuracy: 0.7792\n","Epoch 23/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4379 - accuracy: 0.7964 - val_loss: 0.4392 - val_accuracy: 0.7792\n","Epoch 24/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4366 - accuracy: 0.7932 - val_loss: 0.4421 - val_accuracy: 0.7727\n","Epoch 25/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4357 - accuracy: 0.7997 - val_loss: 0.4416 - val_accuracy: 0.7792\n","Epoch 26/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4351 - accuracy: 0.7932 - val_loss: 0.4421 - val_accuracy: 0.7727\n","Epoch 27/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4353 - accuracy: 0.7915 - val_loss: 0.4397 - val_accuracy: 0.7792\n","Epoch 28/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.4333 - accuracy: 0.7964 - val_loss: 0.4403 - val_accuracy: 0.7792\n","Epoch 29/100\n","20/20 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.8013 - val_loss: 0.4400 - val_accuracy: 0.7727\n","Epoch 30/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.4305 - accuracy: 0.8013 - val_loss: 0.4413 - val_accuracy: 0.7857\n","Epoch 31/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4311 - accuracy: 0.7980 - val_loss: 0.4446 - val_accuracy: 0.7857\n","Epoch 32/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8013 - val_loss: 0.4417 - val_accuracy: 0.7857\n","Epoch 33/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4289 - accuracy: 0.7997 - val_loss: 0.4414 - val_accuracy: 0.7922\n","Epoch 34/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4272 - accuracy: 0.8013 - val_loss: 0.4399 - val_accuracy: 0.7727\n","Epoch 35/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4267 - accuracy: 0.8046 - val_loss: 0.4379 - val_accuracy: 0.7987\n","Epoch 36/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4257 - accuracy: 0.8078 - val_loss: 0.4396 - val_accuracy: 0.7922\n","Epoch 37/100\n","20/20 [==============================] - 0s 12ms/step - loss: 0.4254 - accuracy: 0.8013 - val_loss: 0.4416 - val_accuracy: 0.7857\n","Epoch 38/100\n","20/20 [==============================] - 0s 16ms/step - loss: 0.4243 - accuracy: 0.8046 - val_loss: 0.4432 - val_accuracy: 0.7792\n","Epoch 39/100\n","20/20 [==============================] - 0s 15ms/step - loss: 0.4239 - accuracy: 0.8062 - val_loss: 0.4422 - val_accuracy: 0.7727\n","Epoch 40/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4239 - accuracy: 0.8046 - val_loss: 0.4422 - val_accuracy: 0.7922\n","Epoch 41/100\n","20/20 [==============================] - 0s 14ms/step - loss: 0.4236 - accuracy: 0.8029 - val_loss: 0.4382 - val_accuracy: 0.7857\n","Epoch 42/100\n","20/20 [==============================] - 0s 13ms/step - loss: 0.4220 - accuracy: 0.8078 - val_loss: 0.4420 - val_accuracy: 0.7792\n","Epoch 43/100\n","20/20 [==============================] - 0s 12ms/step - loss: 0.4203 - accuracy: 0.8062 - val_loss: 0.4433 - val_accuracy: 0.7792\n","Epoch 44/100\n","20/20 [==============================] - 0s 20ms/step - loss: 0.4199 - accuracy: 0.8046 - val_loss: 0.4480 - val_accuracy: 0.7597\n","Epoch 45/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4190 - accuracy: 0.8029 - val_loss: 0.4433 - val_accuracy: 0.7727\n","Epoch 46/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4175 - accuracy: 0.8078 - val_loss: 0.4455 - val_accuracy: 0.7727\n","Epoch 47/100\n","20/20 [==============================] - 0s 13ms/step - loss: 0.4181 - accuracy: 0.8111 - val_loss: 0.4460 - val_accuracy: 0.7727\n","Epoch 48/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4172 - accuracy: 0.8094 - val_loss: 0.4449 - val_accuracy: 0.7662\n","Epoch 49/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4175 - accuracy: 0.8062 - val_loss: 0.4413 - val_accuracy: 0.7792\n","Epoch 50/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4152 - accuracy: 0.8094 - val_loss: 0.4451 - val_accuracy: 0.7922\n","Epoch 51/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4158 - accuracy: 0.8078 - val_loss: 0.4452 - val_accuracy: 0.7792\n","Epoch 52/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4148 - accuracy: 0.8160 - val_loss: 0.4480 - val_accuracy: 0.7727\n","Epoch 53/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4139 - accuracy: 0.8078 - val_loss: 0.4494 - val_accuracy: 0.7662\n","Epoch 54/100\n","20/20 [==============================] - 0s 11ms/step - loss: 0.4141 - accuracy: 0.8111 - val_loss: 0.4439 - val_accuracy: 0.7727\n","Epoch 55/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4138 - accuracy: 0.8111 - val_loss: 0.4456 - val_accuracy: 0.7727\n","Epoch 56/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4127 - accuracy: 0.8078 - val_loss: 0.4461 - val_accuracy: 0.7662\n","Epoch 57/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4108 - accuracy: 0.8078 - val_loss: 0.4467 - val_accuracy: 0.7662\n","Epoch 58/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.4106 - accuracy: 0.8094 - val_loss: 0.4445 - val_accuracy: 0.7857\n","Epoch 59/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4098 - accuracy: 0.8192 - val_loss: 0.4474 - val_accuracy: 0.7792\n","Epoch 60/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4090 - accuracy: 0.8127 - val_loss: 0.4470 - val_accuracy: 0.7857\n","Epoch 61/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.4121 - accuracy: 0.8127 - val_loss: 0.4479 - val_accuracy: 0.7792\n","Epoch 62/100\n","20/20 [==============================] - 0s 12ms/step - loss: 0.4133 - accuracy: 0.8111 - val_loss: 0.4482 - val_accuracy: 0.7922\n","Epoch 63/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.4077 - accuracy: 0.8143 - val_loss: 0.4472 - val_accuracy: 0.7727\n","Epoch 64/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4074 - accuracy: 0.8160 - val_loss: 0.4466 - val_accuracy: 0.7727\n","Epoch 65/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4064 - accuracy: 0.8160 - val_loss: 0.4478 - val_accuracy: 0.7727\n","Epoch 66/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4070 - accuracy: 0.8160 - val_loss: 0.4474 - val_accuracy: 0.7857\n","Epoch 67/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4062 - accuracy: 0.8160 - val_loss: 0.4489 - val_accuracy: 0.7857\n","Epoch 68/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4045 - accuracy: 0.8176 - val_loss: 0.4491 - val_accuracy: 0.7792\n","Epoch 69/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8111 - val_loss: 0.4450 - val_accuracy: 0.7662\n","Epoch 70/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4047 - accuracy: 0.8127 - val_loss: 0.4437 - val_accuracy: 0.7922\n","Epoch 71/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4038 - accuracy: 0.8225 - val_loss: 0.4501 - val_accuracy: 0.7662\n","Epoch 72/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4039 - accuracy: 0.8208 - val_loss: 0.4531 - val_accuracy: 0.7662\n","Epoch 73/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8176 - val_loss: 0.4482 - val_accuracy: 0.7792\n","Epoch 74/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4019 - accuracy: 0.8192 - val_loss: 0.4488 - val_accuracy: 0.7792\n","Epoch 75/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.4017 - accuracy: 0.8208 - val_loss: 0.4465 - val_accuracy: 0.7792\n","Epoch 76/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4014 - accuracy: 0.8225 - val_loss: 0.4508 - val_accuracy: 0.7857\n","Epoch 77/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4010 - accuracy: 0.8208 - val_loss: 0.4530 - val_accuracy: 0.7727\n","Epoch 78/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4005 - accuracy: 0.8257 - val_loss: 0.4479 - val_accuracy: 0.7727\n","Epoch 79/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.3976 - accuracy: 0.8208 - val_loss: 0.4486 - val_accuracy: 0.7727\n","Epoch 80/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3990 - accuracy: 0.8225 - val_loss: 0.4483 - val_accuracy: 0.7857\n","Epoch 81/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3978 - accuracy: 0.8241 - val_loss: 0.4498 - val_accuracy: 0.7792\n","Epoch 82/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.3978 - accuracy: 0.8306 - val_loss: 0.4500 - val_accuracy: 0.7857\n","Epoch 83/100\n","20/20 [==============================] - 0s 11ms/step - loss: 0.3965 - accuracy: 0.8274 - val_loss: 0.4476 - val_accuracy: 0.7857\n","Epoch 84/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3961 - accuracy: 0.8290 - val_loss: 0.4480 - val_accuracy: 0.7857\n","Epoch 85/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.3952 - accuracy: 0.8257 - val_loss: 0.4481 - val_accuracy: 0.7792\n","Epoch 86/100\n","20/20 [==============================] - 0s 11ms/step - loss: 0.3940 - accuracy: 0.8257 - val_loss: 0.4484 - val_accuracy: 0.7792\n","Epoch 87/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.3942 - accuracy: 0.8257 - val_loss: 0.4480 - val_accuracy: 0.7727\n","Epoch 88/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.8257 - val_loss: 0.4468 - val_accuracy: 0.7792\n","Epoch 89/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3937 - accuracy: 0.8274 - val_loss: 0.4482 - val_accuracy: 0.7792\n","Epoch 90/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8322 - val_loss: 0.4505 - val_accuracy: 0.7792\n","Epoch 91/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8306 - val_loss: 0.4483 - val_accuracy: 0.7792\n","Epoch 92/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3929 - accuracy: 0.8274 - val_loss: 0.4449 - val_accuracy: 0.7792\n","Epoch 93/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3912 - accuracy: 0.8290 - val_loss: 0.4476 - val_accuracy: 0.7727\n","Epoch 94/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8339 - val_loss: 0.4480 - val_accuracy: 0.7662\n","Epoch 95/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3899 - accuracy: 0.8274 - val_loss: 0.4451 - val_accuracy: 0.7857\n","Epoch 96/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3895 - accuracy: 0.8274 - val_loss: 0.4457 - val_accuracy: 0.7792\n","Epoch 97/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3888 - accuracy: 0.8339 - val_loss: 0.4533 - val_accuracy: 0.7792\n","Epoch 98/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3889 - accuracy: 0.8306 - val_loss: 0.4496 - val_accuracy: 0.7792\n","Epoch 99/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3872 - accuracy: 0.8355 - val_loss: 0.4512 - val_accuracy: 0.7792\n","Epoch 100/100\n","20/20 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8322 - val_loss: 0.4453 - val_accuracy: 0.7857\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7a00c0f45510>"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train, batch_size=32, epochs=100, initial_epoch=6, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Dern2XR3CE6"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4FLXF5zP3Dg5"},"source":["# How many hidden layer?"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1690184969247,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"ugRxfw4f3Fcd"},"outputs":[],"source":["def build_model(hp):\n","  model = Sequential()\n","  model.add(Dense(112, activation='relu', input_dim=8))\n","  for i in range(hp.Int('num_layers', min_value=1, max_value=10)):\n","    model.add(Dense(112,activation='relu'))\n","  model.add(Dense(1,activation='sigmoid'))\n","  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","  return model"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":155,"status":"ok","timestamp":1690185000413,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"SLTVzuqr3-oi"},"outputs":[],"source":["# Tunner object\n","tuner = kt.RandomSearch(build_model, objective = 'val_accuracy', max_trials=5, directory='mydir', project_name='num_layers')"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20012,"status":"ok","timestamp":1690185031548,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"Vv3avJrk4GPV","outputId":"21c8130c-76b8-4864-ad32-88dc4b391fa6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 5 Complete [00h 00m 03s]\n","val_accuracy: 0.7922077775001526\n","\n","Best val_accuracy So Far: 0.8181818127632141\n","Total elapsed time: 00h 00m 20s\n"]}],"source":["tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163,"status":"ok","timestamp":1690185054133,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"9mUvXwKy4I_n","outputId":"17336d09-d02a-4cd5-ca33-796f354e1458"},"outputs":[{"data":{"text/plain":["{'num_layers': 5}"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["tuner.get_best_hyperparameters()[0].values"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":939,"status":"ok","timestamp":1690185098378,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"86NEqcvg4TWl"},"outputs":[],"source":["model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22996,"status":"ok","timestamp":1690185121869,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"gs3_rS244d9a","outputId":"80629d76-60e6-4d85-eda7-76d7aafb4d70"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 7/100\n","20/20 [==============================] - 3s 16ms/step - loss: 0.4602 - accuracy: 0.7850 - val_loss: 0.4420 - val_accuracy: 0.7662\n","Epoch 8/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4438 - accuracy: 0.7899 - val_loss: 0.4373 - val_accuracy: 0.7987\n","Epoch 9/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.7932 - val_loss: 0.4373 - val_accuracy: 0.7857\n","Epoch 10/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8111 - val_loss: 0.4762 - val_accuracy: 0.7597\n","Epoch 11/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4020 - accuracy: 0.8111 - val_loss: 0.4543 - val_accuracy: 0.7987\n","Epoch 12/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.3936 - accuracy: 0.8257 - val_loss: 0.4589 - val_accuracy: 0.8052\n","Epoch 13/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3724 - accuracy: 0.8469 - val_loss: 0.4612 - val_accuracy: 0.7922\n","Epoch 14/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3595 - accuracy: 0.8534 - val_loss: 0.4914 - val_accuracy: 0.7597\n","Epoch 15/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.8306 - val_loss: 0.4690 - val_accuracy: 0.7662\n","Epoch 16/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3458 - accuracy: 0.8469 - val_loss: 0.4760 - val_accuracy: 0.7857\n","Epoch 17/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.3114 - accuracy: 0.8713 - val_loss: 0.5087 - val_accuracy: 0.8052\n","Epoch 18/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.2903 - accuracy: 0.8779 - val_loss: 0.5339 - val_accuracy: 0.7857\n","Epoch 19/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2734 - accuracy: 0.8909 - val_loss: 0.5432 - val_accuracy: 0.7597\n","Epoch 20/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2677 - accuracy: 0.8958 - val_loss: 0.6184 - val_accuracy: 0.7597\n","Epoch 21/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.2461 - accuracy: 0.8990 - val_loss: 0.6837 - val_accuracy: 0.7532\n","Epoch 22/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2509 - accuracy: 0.9023 - val_loss: 0.5971 - val_accuracy: 0.7727\n","Epoch 23/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2299 - accuracy: 0.9055 - val_loss: 0.6686 - val_accuracy: 0.7338\n","Epoch 24/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1984 - accuracy: 0.9186 - val_loss: 0.6495 - val_accuracy: 0.7597\n","Epoch 25/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1636 - accuracy: 0.9316 - val_loss: 0.8016 - val_accuracy: 0.7662\n","Epoch 26/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9414 - val_loss: 0.8612 - val_accuracy: 0.7273\n","Epoch 27/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9609 - val_loss: 1.1179 - val_accuracy: 0.7013\n","Epoch 28/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9446 - val_loss: 0.8503 - val_accuracy: 0.7662\n","Epoch 29/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9332 - val_loss: 0.8888 - val_accuracy: 0.7143\n","Epoch 30/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.9397 - val_loss: 0.8878 - val_accuracy: 0.7208\n","Epoch 31/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1780 - accuracy: 0.9300 - val_loss: 0.7952 - val_accuracy: 0.7273\n","Epoch 32/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1730 - accuracy: 0.9463 - val_loss: 0.8478 - val_accuracy: 0.7208\n","Epoch 33/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0978 - accuracy: 0.9691 - val_loss: 1.0160 - val_accuracy: 0.7078\n","Epoch 34/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9772 - val_loss: 1.1432 - val_accuracy: 0.7208\n","Epoch 35/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9691 - val_loss: 1.0714 - val_accuracy: 0.7468\n","Epoch 36/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9642 - val_loss: 1.1818 - val_accuracy: 0.7078\n","Epoch 37/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9707 - val_loss: 1.0324 - val_accuracy: 0.7338\n","Epoch 38/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0475 - accuracy: 0.9837 - val_loss: 1.2295 - val_accuracy: 0.6883\n","Epoch 39/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0314 - accuracy: 0.9886 - val_loss: 1.3865 - val_accuracy: 0.7273\n","Epoch 40/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 0.9870 - val_loss: 1.3666 - val_accuracy: 0.7597\n","Epoch 41/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1077 - accuracy: 0.9609 - val_loss: 1.3876 - val_accuracy: 0.6948\n","Epoch 42/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1143 - accuracy: 0.9528 - val_loss: 1.2284 - val_accuracy: 0.6818\n","Epoch 43/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9544 - val_loss: 1.1247 - val_accuracy: 0.7273\n","Epoch 44/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0598 - accuracy: 0.9772 - val_loss: 1.0266 - val_accuracy: 0.7338\n","Epoch 45/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 0.9837 - val_loss: 1.2290 - val_accuracy: 0.7273\n","Epoch 46/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.3759 - val_accuracy: 0.7338\n","Epoch 47/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.0132 - accuracy: 0.9951 - val_loss: 1.4622 - val_accuracy: 0.7273\n","Epoch 48/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.5200 - val_accuracy: 0.7208\n","Epoch 49/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 1.6390 - val_accuracy: 0.7143\n","Epoch 50/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 1.6274 - val_accuracy: 0.7338\n","Epoch 51/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 1.7264 - val_accuracy: 0.7208\n","Epoch 52/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 0.9984 - val_loss: 1.7460 - val_accuracy: 0.7273\n","Epoch 53/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.8021 - val_accuracy: 0.7338\n","Epoch 54/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.8792 - val_accuracy: 0.7338\n","Epoch 55/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.9338 - val_accuracy: 0.7338\n","Epoch 56/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.9973 - val_accuracy: 0.7208\n","Epoch 57/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0584 - val_accuracy: 0.7208\n","Epoch 58/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0019 - accuracy: 0.9984 - val_loss: 2.0904 - val_accuracy: 0.7273\n","Epoch 59/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0296 - accuracy: 0.9935 - val_loss: 2.1515 - val_accuracy: 0.7597\n","Epoch 60/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.3023 - accuracy: 0.9349 - val_loss: 1.5705 - val_accuracy: 0.6429\n","Epoch 61/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.2767 - accuracy: 0.9039 - val_loss: 0.6863 - val_accuracy: 0.7468\n","Epoch 62/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9414 - val_loss: 0.7941 - val_accuracy: 0.7273\n","Epoch 63/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9805 - val_loss: 1.1081 - val_accuracy: 0.7208\n","Epoch 64/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0325 - accuracy: 0.9886 - val_loss: 1.2734 - val_accuracy: 0.7273\n","Epoch 65/100\n","20/20 [==============================] - 0s 18ms/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 1.3497 - val_accuracy: 0.7338\n","Epoch 66/100\n","20/20 [==============================] - 0s 13ms/step - loss: 0.0206 - accuracy: 0.9919 - val_loss: 1.5595 - val_accuracy: 0.7143\n","Epoch 67/100\n","20/20 [==============================] - 0s 14ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 1.5803 - val_accuracy: 0.7338\n","Epoch 68/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 1.7678 - val_accuracy: 0.7273\n","Epoch 69/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.9951 - val_loss: 1.6509 - val_accuracy: 0.7208\n","Epoch 70/100\n","20/20 [==============================] - 0s 9ms/step - loss: 0.0176 - accuracy: 0.9951 - val_loss: 1.7175 - val_accuracy: 0.7338\n","Epoch 71/100\n","20/20 [==============================] - 0s 15ms/step - loss: 0.0222 - accuracy: 0.9935 - val_loss: 1.6762 - val_accuracy: 0.7143\n","Epoch 72/100\n","20/20 [==============================] - 0s 10ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 1.6850 - val_accuracy: 0.7013\n","Epoch 73/100\n","20/20 [==============================] - 0s 13ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 1.7953 - val_accuracy: 0.7143\n","Epoch 74/100\n","20/20 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 0.7143\n","Epoch 75/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.9943 - val_accuracy: 0.7143\n","Epoch 76/100\n","20/20 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.0736 - val_accuracy: 0.7143\n","Epoch 77/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1472 - val_accuracy: 0.7143\n","Epoch 78/100\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 2.1926 - val_accuracy: 0.7143\n","Epoch 79/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.1819 - val_accuracy: 0.7078\n","Epoch 80/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 0.9984 - val_loss: 2.2641 - val_accuracy: 0.7143\n","Epoch 81/100\n","20/20 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3077 - val_accuracy: 0.7208\n","Epoch 82/100\n","20/20 [==============================] - 0s 6ms/step - loss: 8.0698e-04 - accuracy: 1.0000 - val_loss: 2.3199 - val_accuracy: 0.7143\n","Epoch 83/100\n","20/20 [==============================] - 0s 6ms/step - loss: 7.1243e-04 - accuracy: 1.0000 - val_loss: 2.3764 - val_accuracy: 0.7143\n","Epoch 84/100\n","20/20 [==============================] - 0s 6ms/step - loss: 4.9803e-04 - accuracy: 1.0000 - val_loss: 2.4180 - val_accuracy: 0.7143\n","Epoch 85/100\n","20/20 [==============================] - 0s 5ms/step - loss: 4.4485e-04 - accuracy: 1.0000 - val_loss: 2.4514 - val_accuracy: 0.7143\n","Epoch 86/100\n","20/20 [==============================] - 0s 6ms/step - loss: 4.0452e-04 - accuracy: 1.0000 - val_loss: 2.4816 - val_accuracy: 0.7143\n","Epoch 87/100\n","20/20 [==============================] - 0s 7ms/step - loss: 4.0186e-04 - accuracy: 1.0000 - val_loss: 2.5142 - val_accuracy: 0.7143\n","Epoch 88/100\n","20/20 [==============================] - 0s 6ms/step - loss: 3.2671e-04 - accuracy: 1.0000 - val_loss: 2.5614 - val_accuracy: 0.7143\n","Epoch 89/100\n","20/20 [==============================] - 0s 6ms/step - loss: 3.4823e-04 - accuracy: 1.0000 - val_loss: 2.5730 - val_accuracy: 0.7143\n","Epoch 90/100\n","20/20 [==============================] - 0s 7ms/step - loss: 2.6590e-04 - accuracy: 1.0000 - val_loss: 2.5964 - val_accuracy: 0.7143\n","Epoch 91/100\n","20/20 [==============================] - 0s 6ms/step - loss: 2.4536e-04 - accuracy: 1.0000 - val_loss: 2.6247 - val_accuracy: 0.7143\n","Epoch 92/100\n","20/20 [==============================] - 0s 6ms/step - loss: 2.4103e-04 - accuracy: 1.0000 - val_loss: 2.6502 - val_accuracy: 0.7208\n","Epoch 93/100\n","20/20 [==============================] - 0s 7ms/step - loss: 2.1423e-04 - accuracy: 1.0000 - val_loss: 2.6675 - val_accuracy: 0.7208\n","Epoch 94/100\n","20/20 [==============================] - 0s 6ms/step - loss: 2.0113e-04 - accuracy: 1.0000 - val_loss: 2.6904 - val_accuracy: 0.7208\n","Epoch 95/100\n","20/20 [==============================] - 0s 7ms/step - loss: 1.9612e-04 - accuracy: 1.0000 - val_loss: 2.7096 - val_accuracy: 0.7143\n","Epoch 96/100\n","20/20 [==============================] - 0s 6ms/step - loss: 1.7428e-04 - accuracy: 1.0000 - val_loss: 2.7324 - val_accuracy: 0.7208\n","Epoch 97/100\n","20/20 [==============================] - 0s 6ms/step - loss: 1.7451e-04 - accuracy: 1.0000 - val_loss: 2.7562 - val_accuracy: 0.7143\n","Epoch 98/100\n","20/20 [==============================] - 0s 6ms/step - loss: 1.7158e-04 - accuracy: 1.0000 - val_loss: 2.7711 - val_accuracy: 0.7143\n","Epoch 99/100\n","20/20 [==============================] - 0s 6ms/step - loss: 1.4723e-04 - accuracy: 1.0000 - val_loss: 2.7967 - val_accuracy: 0.7143\n","Epoch 100/100\n","20/20 [==============================] - 0s 6ms/step - loss: 1.3416e-04 - accuracy: 1.0000 - val_loss: 2.8116 - val_accuracy: 0.7143\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7a00b856e110>"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train, batch_size=32, epochs=100, initial_epoch=6, validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"nizWY1FO4hAP"},"source":["# All hyperparmeter at one go"]},{"cell_type":"markdown","metadata":{"id":"68x9G6tN5BJV"},"source":["Task::\n","\n","1. First task to to give number of hidden layer and then number of neurons\n","\n","2."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":158,"status":"ok","timestamp":1690186014639,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"Dfzs_5on4eTh"},"outputs":[],"source":["def build_model(hp):\n","  model = Sequential()\n","  counter = 0\n","\n","  for i in range(hp.Int('num_layers', min_value=1, max_value=10)):\n","    if counter == 0: # 1st layer\n","      model.add(Dense(hp.Int('units' + str(i), min_value=8, max_value=128,step=8),\n","                      activation = hp.Choice('activation' + str(i), values = ['relu', 'tanh', 'sigmoid','elu','leaky_relu','softmax','linear']),input_dim=8))\n","      model.add(Dropout(hp.Choice('dropout' + str(i), values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])))\n","    else :\n","        model.add(Dense(hp.Int('units' + str(i), min_value=8, max_value=128,step=8),\n","                      activation = hp.Choice('activation' + str(i), values = ['relu', 'tanh', 'sigmoid','elu','leaky_relu','softmax','linear']),))\n","        model.add(Dropout(hp.Choice('dropout' + str(i), values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])))\n","    counter +=1\n","  model.add(Dense(1, activation='sigmoid')) # last layer\n","  model.compile(optimizer=hp.Choice('optimizer', values=['rmsprop','adam','sgd','nadam','adadelta','adagrad']), loss='binary_crossentropy', metrics=['accuracy'])\n","  return model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":175,"status":"ok","timestamp":1690186017335,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"y_ono8-V7akl"},"outputs":[],"source":["tuner = kt.RandomSearch(build_model, objective = 'val_accuracy',\n","                        max_trials=10, directory='mydir', project_name='final_1')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35457,"status":"ok","timestamp":1690186054691,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"Ix9qMERP7f8X","outputId":"975ff874-67db-48d5-9d46-a05cbcc9ac8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 10 Complete [00h 00m 02s]\n","val_accuracy: 0.6688311696052551\n","\n","Best val_accuracy So Far: 0.7597402334213257\n","Total elapsed time: 00h 00m 35s\n"]}],"source":["tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1690186060220,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"8u2atrF67hS8","outputId":"51ef11a8-66a0-40fa-e80e-8b4dedc6fd08"},"outputs":[{"data":{"text/plain":["{'num_layers': 1,\n"," 'units0': 32,\n"," 'activation0': 'tanh',\n"," 'dropout0': 0.1,\n"," 'optimizer': 'rmsprop',\n"," 'units1': 40,\n"," 'activation1': 'leaky_relu',\n"," 'dropout1': 0.9,\n"," 'units2': 48,\n"," 'activation2': 'elu',\n"," 'dropout2': 0.3,\n"," 'units3': 8,\n"," 'activation3': 'tanh',\n"," 'dropout3': 0.6,\n"," 'units4': 56,\n"," 'activation4': 'elu',\n"," 'dropout4': 0.6,\n"," 'units5': 48,\n"," 'activation5': 'linear',\n"," 'dropout5': 0.2,\n"," 'units6': 88,\n"," 'activation6': 'leaky_relu',\n"," 'dropout6': 0.1,\n"," 'units7': 80,\n"," 'activation7': 'sigmoid',\n"," 'dropout7': 0.3,\n"," 'units8': 8,\n"," 'activation8': 'tanh',\n"," 'dropout8': 0.5}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tuner.get_best_hyperparameters()[0].values"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":599,"status":"ok","timestamp":1690186063935,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"kWvoxNrD7i7W"},"outputs":[],"source":["model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25660,"status":"ok","timestamp":1690186254997,"user":{"displayName":"Akash Joyboy","userId":"11664201711311323948"},"user_tz":420},"id":"4y5xJOfl7mYW","outputId":"57143223-75f5-40de-a266-1a7438c9b8c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 6/200\n","20/20 [==============================] - 2s 40ms/step - loss: 0.5249 - accuracy: 0.7508 - val_loss: 0.5069 - val_accuracy: 0.7468\n","Epoch 7/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.5142 - accuracy: 0.7622 - val_loss: 0.4932 - val_accuracy: 0.7597\n","Epoch 8/200\n","20/20 [==============================] - 0s 18ms/step - loss: 0.5018 - accuracy: 0.7541 - val_loss: 0.4812 - val_accuracy: 0.7662\n","Epoch 9/200\n","20/20 [==============================] - 0s 15ms/step - loss: 0.4970 - accuracy: 0.7508 - val_loss: 0.4747 - val_accuracy: 0.7792\n","Epoch 10/200\n","20/20 [==============================] - 0s 11ms/step - loss: 0.4920 - accuracy: 0.7687 - val_loss: 0.4668 - val_accuracy: 0.7792\n","Epoch 11/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4855 - accuracy: 0.7638 - val_loss: 0.4607 - val_accuracy: 0.7922\n","Epoch 12/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4862 - accuracy: 0.7704 - val_loss: 0.4553 - val_accuracy: 0.7987\n","Epoch 13/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.7752 - val_loss: 0.4537 - val_accuracy: 0.7987\n","Epoch 14/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4792 - accuracy: 0.7769 - val_loss: 0.4519 - val_accuracy: 0.7987\n","Epoch 15/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4781 - accuracy: 0.7818 - val_loss: 0.4501 - val_accuracy: 0.7922\n","Epoch 16/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4718 - accuracy: 0.7704 - val_loss: 0.4481 - val_accuracy: 0.7922\n","Epoch 17/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4758 - accuracy: 0.7638 - val_loss: 0.4473 - val_accuracy: 0.7922\n","Epoch 18/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4730 - accuracy: 0.7752 - val_loss: 0.4475 - val_accuracy: 0.7857\n","Epoch 19/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4749 - accuracy: 0.7704 - val_loss: 0.4463 - val_accuracy: 0.7857\n","Epoch 20/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4696 - accuracy: 0.7801 - val_loss: 0.4451 - val_accuracy: 0.7857\n","Epoch 21/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4778 - accuracy: 0.7736 - val_loss: 0.4438 - val_accuracy: 0.7987\n","Epoch 22/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.7785 - val_loss: 0.4436 - val_accuracy: 0.7987\n","Epoch 23/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.7769 - val_loss: 0.4422 - val_accuracy: 0.7987\n","Epoch 24/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4690 - accuracy: 0.7899 - val_loss: 0.4425 - val_accuracy: 0.7987\n","Epoch 25/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4732 - accuracy: 0.7720 - val_loss: 0.4417 - val_accuracy: 0.7922\n","Epoch 26/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4735 - accuracy: 0.7606 - val_loss: 0.4417 - val_accuracy: 0.7987\n","Epoch 27/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.7752 - val_loss: 0.4425 - val_accuracy: 0.7922\n","Epoch 28/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.7720 - val_loss: 0.4425 - val_accuracy: 0.7922\n","Epoch 29/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4698 - accuracy: 0.7801 - val_loss: 0.4434 - val_accuracy: 0.8052\n","Epoch 30/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4771 - accuracy: 0.7769 - val_loss: 0.4440 - val_accuracy: 0.7987\n","Epoch 31/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4666 - accuracy: 0.7915 - val_loss: 0.4441 - val_accuracy: 0.7987\n","Epoch 32/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4697 - accuracy: 0.7785 - val_loss: 0.4433 - val_accuracy: 0.7922\n","Epoch 33/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4661 - accuracy: 0.7736 - val_loss: 0.4438 - val_accuracy: 0.7922\n","Epoch 34/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.7752 - val_loss: 0.4442 - val_accuracy: 0.7922\n","Epoch 35/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.7818 - val_loss: 0.4444 - val_accuracy: 0.7857\n","Epoch 36/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4715 - accuracy: 0.7785 - val_loss: 0.4440 - val_accuracy: 0.7792\n","Epoch 37/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.7769 - val_loss: 0.4440 - val_accuracy: 0.7987\n","Epoch 38/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4635 - accuracy: 0.7899 - val_loss: 0.4449 - val_accuracy: 0.7987\n","Epoch 39/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7785 - val_loss: 0.4441 - val_accuracy: 0.7987\n","Epoch 40/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4663 - accuracy: 0.7850 - val_loss: 0.4436 - val_accuracy: 0.7987\n","Epoch 41/200\n","20/20 [==============================] - 0s 11ms/step - loss: 0.4656 - accuracy: 0.7834 - val_loss: 0.4430 - val_accuracy: 0.8052\n","Epoch 42/200\n","20/20 [==============================] - 0s 11ms/step - loss: 0.4627 - accuracy: 0.7818 - val_loss: 0.4433 - val_accuracy: 0.8052\n","Epoch 43/200\n","20/20 [==============================] - 0s 14ms/step - loss: 0.4640 - accuracy: 0.7915 - val_loss: 0.4432 - val_accuracy: 0.7922\n","Epoch 44/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4658 - accuracy: 0.7704 - val_loss: 0.4431 - val_accuracy: 0.7922\n","Epoch 45/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4618 - accuracy: 0.7752 - val_loss: 0.4428 - val_accuracy: 0.7922\n","Epoch 46/200\n","20/20 [==============================] - 0s 9ms/step - loss: 0.4679 - accuracy: 0.7785 - val_loss: 0.4434 - val_accuracy: 0.7857\n","Epoch 47/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4699 - accuracy: 0.7704 - val_loss: 0.4424 - val_accuracy: 0.7922\n","Epoch 48/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4641 - accuracy: 0.7850 - val_loss: 0.4422 - val_accuracy: 0.7922\n","Epoch 49/200\n","20/20 [==============================] - 0s 12ms/step - loss: 0.4605 - accuracy: 0.7769 - val_loss: 0.4414 - val_accuracy: 0.7792\n","Epoch 50/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4596 - accuracy: 0.7785 - val_loss: 0.4412 - val_accuracy: 0.7922\n","Epoch 51/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4700 - accuracy: 0.7785 - val_loss: 0.4409 - val_accuracy: 0.7987\n","Epoch 52/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4621 - accuracy: 0.7736 - val_loss: 0.4412 - val_accuracy: 0.8052\n","Epoch 53/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4603 - accuracy: 0.7915 - val_loss: 0.4421 - val_accuracy: 0.7987\n","Epoch 54/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4644 - accuracy: 0.7818 - val_loss: 0.4426 - val_accuracy: 0.7987\n","Epoch 55/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4687 - accuracy: 0.7818 - val_loss: 0.4432 - val_accuracy: 0.7922\n","Epoch 56/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4623 - accuracy: 0.7850 - val_loss: 0.4424 - val_accuracy: 0.7987\n","Epoch 57/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4632 - accuracy: 0.7785 - val_loss: 0.4432 - val_accuracy: 0.7987\n","Epoch 58/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4556 - accuracy: 0.7801 - val_loss: 0.4433 - val_accuracy: 0.7922\n","Epoch 59/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4613 - accuracy: 0.7769 - val_loss: 0.4433 - val_accuracy: 0.7922\n","Epoch 60/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4607 - accuracy: 0.7818 - val_loss: 0.4441 - val_accuracy: 0.7987\n","Epoch 61/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4662 - accuracy: 0.7785 - val_loss: 0.4428 - val_accuracy: 0.8052\n","Epoch 62/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4617 - accuracy: 0.7883 - val_loss: 0.4436 - val_accuracy: 0.7987\n","Epoch 63/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4664 - accuracy: 0.7785 - val_loss: 0.4423 - val_accuracy: 0.7987\n","Epoch 64/200\n","20/20 [==============================] - 0s 10ms/step - loss: 0.4605 - accuracy: 0.7769 - val_loss: 0.4415 - val_accuracy: 0.7987\n","Epoch 65/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4666 - accuracy: 0.7769 - val_loss: 0.4408 - val_accuracy: 0.7922\n","Epoch 66/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4575 - accuracy: 0.7899 - val_loss: 0.4413 - val_accuracy: 0.7987\n","Epoch 67/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4611 - accuracy: 0.7769 - val_loss: 0.4401 - val_accuracy: 0.7987\n","Epoch 68/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4589 - accuracy: 0.7785 - val_loss: 0.4401 - val_accuracy: 0.7922\n","Epoch 69/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4528 - accuracy: 0.7785 - val_loss: 0.4401 - val_accuracy: 0.7987\n","Epoch 70/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4538 - accuracy: 0.7866 - val_loss: 0.4404 - val_accuracy: 0.7987\n","Epoch 71/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4594 - accuracy: 0.7834 - val_loss: 0.4399 - val_accuracy: 0.7922\n","Epoch 72/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4577 - accuracy: 0.7785 - val_loss: 0.4400 - val_accuracy: 0.8052\n","Epoch 73/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4588 - accuracy: 0.7899 - val_loss: 0.4406 - val_accuracy: 0.7987\n","Epoch 74/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4609 - accuracy: 0.7866 - val_loss: 0.4390 - val_accuracy: 0.7987\n","Epoch 75/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4588 - accuracy: 0.7785 - val_loss: 0.4398 - val_accuracy: 0.7987\n","Epoch 76/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4640 - accuracy: 0.7801 - val_loss: 0.4397 - val_accuracy: 0.7987\n","Epoch 77/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4618 - accuracy: 0.7915 - val_loss: 0.4389 - val_accuracy: 0.7922\n","Epoch 78/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4522 - accuracy: 0.7769 - val_loss: 0.4380 - val_accuracy: 0.7857\n","Epoch 79/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4632 - accuracy: 0.7671 - val_loss: 0.4376 - val_accuracy: 0.7857\n","Epoch 80/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4533 - accuracy: 0.7818 - val_loss: 0.4363 - val_accuracy: 0.7922\n","Epoch 81/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4531 - accuracy: 0.7818 - val_loss: 0.4366 - val_accuracy: 0.7922\n","Epoch 82/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4496 - accuracy: 0.7850 - val_loss: 0.4373 - val_accuracy: 0.7987\n","Epoch 83/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4508 - accuracy: 0.7818 - val_loss: 0.4382 - val_accuracy: 0.8052\n","Epoch 84/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4519 - accuracy: 0.7883 - val_loss: 0.4386 - val_accuracy: 0.7987\n","Epoch 85/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4589 - accuracy: 0.7818 - val_loss: 0.4383 - val_accuracy: 0.8052\n","Epoch 86/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4525 - accuracy: 0.7899 - val_loss: 0.4377 - val_accuracy: 0.8052\n","Epoch 87/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7850 - val_loss: 0.4360 - val_accuracy: 0.8117\n","Epoch 88/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.7785 - val_loss: 0.4360 - val_accuracy: 0.8052\n","Epoch 89/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.7801 - val_loss: 0.4371 - val_accuracy: 0.8052\n","Epoch 90/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7801 - val_loss: 0.4367 - val_accuracy: 0.8052\n","Epoch 91/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4514 - accuracy: 0.7818 - val_loss: 0.4368 - val_accuracy: 0.8117\n","Epoch 92/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.7834 - val_loss: 0.4378 - val_accuracy: 0.8052\n","Epoch 93/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7818 - val_loss: 0.4371 - val_accuracy: 0.8052\n","Epoch 94/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7834 - val_loss: 0.4380 - val_accuracy: 0.7922\n","Epoch 95/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4532 - accuracy: 0.7883 - val_loss: 0.4379 - val_accuracy: 0.7922\n","Epoch 96/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7834 - val_loss: 0.4368 - val_accuracy: 0.8052\n","Epoch 97/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.7834 - val_loss: 0.4364 - val_accuracy: 0.8052\n","Epoch 98/200\n","20/20 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7801 - val_loss: 0.4369 - val_accuracy: 0.7987\n","Epoch 99/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.7834 - val_loss: 0.4376 - val_accuracy: 0.7987\n","Epoch 100/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.7899 - val_loss: 0.4366 - val_accuracy: 0.7922\n","Epoch 101/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7850 - val_loss: 0.4370 - val_accuracy: 0.7987\n","Epoch 102/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4556 - accuracy: 0.7866 - val_loss: 0.4356 - val_accuracy: 0.8052\n","Epoch 103/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7866 - val_loss: 0.4358 - val_accuracy: 0.7922\n","Epoch 104/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.7752 - val_loss: 0.4359 - val_accuracy: 0.7922\n","Epoch 105/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4527 - accuracy: 0.7818 - val_loss: 0.4371 - val_accuracy: 0.7922\n","Epoch 106/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.7834 - val_loss: 0.4374 - val_accuracy: 0.7922\n","Epoch 107/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.7801 - val_loss: 0.4361 - val_accuracy: 0.7987\n","Epoch 108/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4506 - accuracy: 0.7785 - val_loss: 0.4368 - val_accuracy: 0.8052\n","Epoch 109/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7687 - val_loss: 0.4363 - val_accuracy: 0.7987\n","Epoch 110/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.7915 - val_loss: 0.4350 - val_accuracy: 0.8052\n","Epoch 111/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4440 - accuracy: 0.7915 - val_loss: 0.4363 - val_accuracy: 0.8052\n","Epoch 112/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4452 - accuracy: 0.7818 - val_loss: 0.4354 - val_accuracy: 0.7987\n","Epoch 113/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4524 - accuracy: 0.7834 - val_loss: 0.4362 - val_accuracy: 0.7922\n","Epoch 114/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4535 - accuracy: 0.7850 - val_loss: 0.4375 - val_accuracy: 0.8052\n","Epoch 115/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4481 - accuracy: 0.7883 - val_loss: 0.4359 - val_accuracy: 0.8052\n","Epoch 116/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4526 - accuracy: 0.7818 - val_loss: 0.4356 - val_accuracy: 0.8052\n","Epoch 117/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4411 - accuracy: 0.7915 - val_loss: 0.4365 - val_accuracy: 0.7987\n","Epoch 118/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4484 - accuracy: 0.7883 - val_loss: 0.4371 - val_accuracy: 0.7922\n","Epoch 119/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4378 - accuracy: 0.7818 - val_loss: 0.4359 - val_accuracy: 0.7922\n","Epoch 120/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4471 - accuracy: 0.7818 - val_loss: 0.4357 - val_accuracy: 0.7987\n","Epoch 121/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4516 - accuracy: 0.7818 - val_loss: 0.4336 - val_accuracy: 0.8052\n","Epoch 122/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4519 - accuracy: 0.7932 - val_loss: 0.4352 - val_accuracy: 0.7987\n","Epoch 123/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7752 - val_loss: 0.4355 - val_accuracy: 0.7922\n","Epoch 124/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7850 - val_loss: 0.4361 - val_accuracy: 0.7987\n","Epoch 125/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4528 - accuracy: 0.7834 - val_loss: 0.4365 - val_accuracy: 0.7922\n","Epoch 126/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4522 - accuracy: 0.7752 - val_loss: 0.4363 - val_accuracy: 0.7922\n","Epoch 127/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4497 - accuracy: 0.7736 - val_loss: 0.4349 - val_accuracy: 0.7987\n","Epoch 128/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.7785 - val_loss: 0.4336 - val_accuracy: 0.7987\n","Epoch 129/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.7801 - val_loss: 0.4338 - val_accuracy: 0.7922\n","Epoch 130/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7866 - val_loss: 0.4323 - val_accuracy: 0.7922\n","Epoch 131/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4515 - accuracy: 0.7899 - val_loss: 0.4329 - val_accuracy: 0.7922\n","Epoch 132/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.7866 - val_loss: 0.4332 - val_accuracy: 0.7922\n","Epoch 133/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4501 - accuracy: 0.7769 - val_loss: 0.4341 - val_accuracy: 0.7922\n","Epoch 134/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4430 - accuracy: 0.7883 - val_loss: 0.4342 - val_accuracy: 0.7922\n","Epoch 135/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4447 - accuracy: 0.7899 - val_loss: 0.4342 - val_accuracy: 0.7922\n","Epoch 136/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7850 - val_loss: 0.4333 - val_accuracy: 0.7922\n","Epoch 137/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.7899 - val_loss: 0.4333 - val_accuracy: 0.7922\n","Epoch 138/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.7883 - val_loss: 0.4342 - val_accuracy: 0.7922\n","Epoch 139/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4528 - accuracy: 0.7801 - val_loss: 0.4344 - val_accuracy: 0.7922\n","Epoch 140/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.7818 - val_loss: 0.4359 - val_accuracy: 0.7922\n","Epoch 141/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.7915 - val_loss: 0.4342 - val_accuracy: 0.7922\n","Epoch 142/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.7801 - val_loss: 0.4343 - val_accuracy: 0.7857\n","Epoch 143/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4466 - accuracy: 0.7818 - val_loss: 0.4342 - val_accuracy: 0.7857\n","Epoch 144/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4431 - accuracy: 0.7850 - val_loss: 0.4333 - val_accuracy: 0.7922\n","Epoch 145/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4502 - accuracy: 0.7850 - val_loss: 0.4320 - val_accuracy: 0.7987\n","Epoch 146/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7932 - val_loss: 0.4335 - val_accuracy: 0.7922\n","Epoch 147/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4465 - accuracy: 0.7736 - val_loss: 0.4325 - val_accuracy: 0.7922\n","Epoch 148/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4506 - accuracy: 0.7866 - val_loss: 0.4315 - val_accuracy: 0.7922\n","Epoch 149/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4436 - accuracy: 0.7818 - val_loss: 0.4335 - val_accuracy: 0.7922\n","Epoch 150/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.7883 - val_loss: 0.4333 - val_accuracy: 0.7922\n","Epoch 151/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7850 - val_loss: 0.4333 - val_accuracy: 0.7987\n","Epoch 152/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.7850 - val_loss: 0.4335 - val_accuracy: 0.7922\n","Epoch 153/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.7883 - val_loss: 0.4343 - val_accuracy: 0.8052\n","Epoch 154/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4401 - accuracy: 0.7964 - val_loss: 0.4337 - val_accuracy: 0.7922\n","Epoch 155/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8046 - val_loss: 0.4347 - val_accuracy: 0.7922\n","Epoch 156/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4452 - accuracy: 0.7850 - val_loss: 0.4345 - val_accuracy: 0.7922\n","Epoch 157/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4469 - accuracy: 0.7834 - val_loss: 0.4337 - val_accuracy: 0.7922\n","Epoch 158/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.7785 - val_loss: 0.4358 - val_accuracy: 0.7857\n","Epoch 159/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4390 - accuracy: 0.7850 - val_loss: 0.4357 - val_accuracy: 0.7792\n","Epoch 160/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4450 - accuracy: 0.7850 - val_loss: 0.4355 - val_accuracy: 0.7857\n","Epoch 161/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.7736 - val_loss: 0.4327 - val_accuracy: 0.7922\n","Epoch 162/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4335 - accuracy: 0.7866 - val_loss: 0.4335 - val_accuracy: 0.7857\n","Epoch 163/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.7785 - val_loss: 0.4334 - val_accuracy: 0.7857\n","Epoch 164/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4402 - accuracy: 0.7932 - val_loss: 0.4322 - val_accuracy: 0.7922\n","Epoch 165/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.7850 - val_loss: 0.4332 - val_accuracy: 0.7922\n","Epoch 166/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.7883 - val_loss: 0.4332 - val_accuracy: 0.7857\n","Epoch 167/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.7801 - val_loss: 0.4324 - val_accuracy: 0.7922\n","Epoch 168/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7850 - val_loss: 0.4326 - val_accuracy: 0.7857\n","Epoch 169/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4377 - accuracy: 0.7801 - val_loss: 0.4326 - val_accuracy: 0.7857\n","Epoch 170/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.7899 - val_loss: 0.4336 - val_accuracy: 0.7857\n","Epoch 171/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7883 - val_loss: 0.4332 - val_accuracy: 0.7922\n","Epoch 172/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4394 - accuracy: 0.7818 - val_loss: 0.4334 - val_accuracy: 0.7857\n","Epoch 173/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4407 - accuracy: 0.7850 - val_loss: 0.4333 - val_accuracy: 0.7857\n","Epoch 174/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.7915 - val_loss: 0.4319 - val_accuracy: 0.7922\n","Epoch 175/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4342 - accuracy: 0.8046 - val_loss: 0.4351 - val_accuracy: 0.7857\n","Epoch 176/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.7818 - val_loss: 0.4354 - val_accuracy: 0.7857\n","Epoch 177/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4390 - accuracy: 0.7899 - val_loss: 0.4348 - val_accuracy: 0.7792\n","Epoch 178/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4353 - accuracy: 0.7948 - val_loss: 0.4329 - val_accuracy: 0.7922\n","Epoch 179/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.7948 - val_loss: 0.4343 - val_accuracy: 0.7857\n","Epoch 180/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.7964 - val_loss: 0.4340 - val_accuracy: 0.7792\n","Epoch 181/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7948 - val_loss: 0.4343 - val_accuracy: 0.7792\n","Epoch 182/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4368 - accuracy: 0.7948 - val_loss: 0.4347 - val_accuracy: 0.7857\n","Epoch 183/200\n","20/20 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.7899 - val_loss: 0.4334 - val_accuracy: 0.7857\n","Epoch 184/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4408 - accuracy: 0.7834 - val_loss: 0.4330 - val_accuracy: 0.7857\n","Epoch 185/200\n","20/20 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.7948 - val_loss: 0.4348 - val_accuracy: 0.7792\n","Epoch 186/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4321 - accuracy: 0.8013 - val_loss: 0.4343 - val_accuracy: 0.7857\n","Epoch 187/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4379 - accuracy: 0.7801 - val_loss: 0.4342 - val_accuracy: 0.7792\n","Epoch 188/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.7948 - val_loss: 0.4355 - val_accuracy: 0.7792\n","Epoch 189/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4366 - accuracy: 0.7866 - val_loss: 0.4341 - val_accuracy: 0.7857\n","Epoch 190/200\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4407 - accuracy: 0.7801 - val_loss: 0.4340 - val_accuracy: 0.7792\n","Epoch 191/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4396 - accuracy: 0.7997 - val_loss: 0.4329 - val_accuracy: 0.7857\n","Epoch 192/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4399 - accuracy: 0.7818 - val_loss: 0.4337 - val_accuracy: 0.7857\n","Epoch 193/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4406 - accuracy: 0.7818 - val_loss: 0.4336 - val_accuracy: 0.7857\n","Epoch 194/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4455 - accuracy: 0.7834 - val_loss: 0.4342 - val_accuracy: 0.7857\n","Epoch 195/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4293 - accuracy: 0.7915 - val_loss: 0.4342 - val_accuracy: 0.7792\n","Epoch 196/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4371 - accuracy: 0.7948 - val_loss: 0.4331 - val_accuracy: 0.7857\n","Epoch 197/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4419 - accuracy: 0.7948 - val_loss: 0.4327 - val_accuracy: 0.7857\n","Epoch 198/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4379 - accuracy: 0.7883 - val_loss: 0.4324 - val_accuracy: 0.7922\n","Epoch 199/200\n","20/20 [==============================] - 0s 7ms/step - loss: 0.4369 - accuracy: 0.7850 - val_loss: 0.4343 - val_accuracy: 0.7792\n","Epoch 200/200\n","20/20 [==============================] - 0s 8ms/step - loss: 0.4386 - accuracy: 0.7850 - val_loss: 0.4360 - val_accuracy: 0.7857\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7c46f2a4a9b0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train, batch_size=32, epochs=200, initial_epoch=5, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CYP6YgU8yTs"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMc3uoRNta2rzNvRKd/uykB","mount_file_id":"1hP50lIAVt6ZiGb-resediWWAi4kMn7mJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
